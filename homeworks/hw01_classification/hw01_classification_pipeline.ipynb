{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b656a4266174b009",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "WsIpsmveSDpL"
      },
      "source": [
        "### 1. Reading the data\n",
        "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-86e0de040aac317a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "Ge5dQVEySDpJ"
      },
      "source": [
        "Home assignment №1\n",
        "This assignment consists of several parts. You are supposed to make some transformations, train several models, estimate their quality of the models and explain your results.\n",
        "\n",
        "*Don't hesitate to ask questions, it's a good practice.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7mmo0npzSDpM",
        "outputId": "6569939d-305d-4471-cb43-a1d980a59302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 12:50:39--  https://raw.githubusercontent.com/girafe-ai/ml-course/23f_ptml/homeworks/hw01_classification/car_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58374 (57K) [text/plain]\n",
            "Saving to: ‘car_data.csv’\n",
            "\n",
            "\rcar_data.csv          0%[                    ]       0  --.-KB/s               \rcar_data.csv        100%[===================>]  57.01K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-10 12:50:39 (4.36 MB/s) - ‘car_data.csv’ saved [58374/58374]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If on colab, uncomment the following lines\n",
        "\n",
        "! wget https://raw.githubusercontent.com/girafe-ai/ml-course/23f_ptml/homeworks/hw01_classification/car_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-eebac6bfdf73d0bc",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "n59VLFcdSDpM",
        "outputId": "abae9f5d-9aea-415b-bda3-8ac719661cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(846, 19) (846,)\n",
            "(549, 19) (549,) (297, 19) (297,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
        "data = dataset[:, :-1].astype(int)\n",
        "target = dataset[:, -1]\n",
        "\n",
        "print(data.shape, target.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-88b1a0f688568f2c",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "jN5qOlUXSDpN"
      },
      "source": [
        "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei3uBKYDSDpN"
      },
      "outputs": [],
      "source": [
        "X_train_pd = pd.DataFrame(X_train)\n",
        "\n",
        "# First 15 rows of our dataset.\n",
        "X_train_pd.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-98e7d91d77d65fcf",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "h4zvzlYzSDpO"
      },
      "source": [
        "Methods `describe` and `info` deliver some useful information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4vXiBkJSDpO"
      },
      "outputs": [],
      "source": [
        "X_train_pd.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD0lD0POSDpO"
      },
      "outputs": [],
      "source": [
        "X_train_pd.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be844269be69c387",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "gdiPLZlPSDpO"
      },
      "source": [
        "### 2. Machine Learning pipeline\n",
        "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQs_m9jRSDpO"
      },
      "source": [
        "#### 2.0. Data preprocessing\n",
        "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a1514aa189a49fca",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "wut261ndSDpP"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_scaled = scaler.transform(X_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWvsf5gKSDpP"
      },
      "source": [
        "#### 2.1. Basic logistic regression\n",
        "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
        "\n",
        "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-1dd5ad5d0845cbbb",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "2ZSheft2SDpP",
        "outputId": "c04e1f56-06d1-4060-a2c8-2cf20a8fbe7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.87481203e-02, 2.53232798e-01, 3.45390906e-01, 3.32628175e-01],\n",
              "       [3.16329145e-02, 5.72195370e-01, 3.95169935e-01, 1.00178037e-03],\n",
              "       [2.39521189e-05, 2.88823212e-02, 9.70920083e-01, 1.73643717e-04],\n",
              "       ...,\n",
              "       [7.15742773e-04, 1.82190175e-01, 8.16171583e-01, 9.22500040e-04],\n",
              "       [9.95361587e-01, 4.22955616e-03, 4.07315301e-04, 1.54120462e-06],\n",
              "       [6.33473709e-01, 2.71956782e-01, 9.25307916e-02, 2.03871695e-03]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='saga', tol=1e-3, max_iter=500)\n",
        "clf = lr.fit(X_scaled, y_train)\n",
        "\n",
        "#clf.predict(X_test_scaled)\n",
        "#clf.score(X_test_scaled, y_test)\n",
        "clf.predict_proba(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjjQHhaNSDpP"
      },
      "outputs": [],
      "source": [
        "# You might use this command to install scikit-plot.\n",
        "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding\n",
        "# virtual environment instead\n",
        "\n",
        "! pip install scikit-plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1vTzpwJSDpP"
      },
      "source": [
        "**Note: From `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d28b58a35c94e988",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "3Cgbr7wLSDpP"
      },
      "source": [
        "#### 2.2. Naive Bayes classifier on original or preprocessed data.\n",
        "* Find optimal hyperparameters for Naive Bayes classifier (select the appropriate one from sklearn) with cross-validation on the  `train` data.\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-12d53ea45258fa82",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "UTCTZ8qeSDpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\n",
        "\n",
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4fbf16c64076e139",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "VCZghw0gSDpP"
      },
      "source": [
        "#### 2.3. kNN classifier\n",
        "* Now \"train\" a kNN model on the `train` data. Use any transformations if needed.\n",
        "\n",
        "* Measure the model quality using the same metrics you used above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-748ed20b51c67fab",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "9s-A_bdVSDpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier¶\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ3uft9ISDpQ"
      },
      "source": [
        "#### 2.4. Decision tree (start after the week04!)\n",
        "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
        "\n",
        "* Measure the model quality using the same metrics you used above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9eadd4d8a03ae67a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "n7Dd_k_zSDpQ"
      },
      "source": [
        "#### 2.5. Bagging (start after week04)\n",
        "Here starts the ensembling part.\n",
        "\n",
        "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
        "\n",
        "We will build two ensembles: of logistic regressions and of decision trees.\n",
        "\n",
        "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
        "\n",
        "\n",
        "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
        "\n",
        "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
        "\n",
        "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
        "\n",
        "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
        "\n",
        "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-8fc95a2b206bdae1",
          "locked": false,
          "points": 35,
          "schema_version": 2,
          "solution": true
        },
        "id": "kfGRqYnESDpQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjoAb40pSDpQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-241b7691ab44cbfb",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "xA8A8_Z0SDpQ"
      },
      "source": [
        "#### 2.7. Random Forest (start after week04)\n",
        "Now we will work with the Random Forest (its `sklearn` implementation).\n",
        "\n",
        "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
        "\n",
        "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-888755d0f3d91620",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "HiSFIljZSDpQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-99191c0852538d4d",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "ADfzF_mnSDpQ"
      },
      "source": [
        "#### 2.8. Learning curve\n",
        "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
        "\n",
        "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
        "\n",
        "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
        "\n",
        "* Analyse the final plot. Can you make any conlusions using it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e39bc7e7dff61ff9",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "89mvQTYnSDpQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}